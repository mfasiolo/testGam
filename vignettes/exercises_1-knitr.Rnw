%
% TODO
% 2. During the talk, show the select = TRUE argument
%

\documentclass[11pt,british]{article}
\usepackage{bm}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.0cm,rmargin=2.0cm}
\usepackage{url}
%\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{color,colortbl}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{amssymb,amsmath}
\usepackage{chngcntr}
\usepackage{changepage} 
\usepackage[round,authoryear]{natbib}
\bibliographystyle{apalike}
\usepackage{tipa}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}

\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=true,pdfborder={0 0 1},backref=false,colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}   
\setcounter{totalnumber}{50}
\setcounter{topnumber}{50}
\setcounter{bottomnumber}{50}

\setlength{\parindent}{0in}
\begin{document}

<<include=FALSE>>=
library(knitr)
opts_chunk$set(
concordance=TRUE
)
@


<<setup,echo=FALSE,cache=FALSE,eval=TRUE>>=
setwd("~/Desktop/All/Dropbox/Work/Notes/Slides/qgam/2019_EDF_mgcViz/exercises")
options(prompt = " ", continue = " ")
require(knitr)
options(replace.assign=TRUE,show.signif.stars=FALSE,width=100)
options(lattice.theme = function() canonical.theme("pdf", color = TRUE))
opts_chunk$set(warning=FALSE,cache=FALSE,size='small',tidy=FALSE,
               fig.path='figs/fig-',out.truncate=90,comment=NA,
               fig.show='hold',tab.align="center")
hook_output <- knit_hooks$get("output")
knit_hooks$set(output=function(x, options) {
  if (options$results != "asis") {
    # Split string into separate lines.
    x <- unlist(stringr::str_split(x, "\n")) 
    # Truncate each line to length specified.
    if (!is.null(m <- options$out.truncate)) {
      len <- nchar(x) 
      x[len>m] <- paste0(substr(x[len>m], 0, m-3), "...")
    }
    # Paste lines back together.
    x <- paste (x, collapse= "\n")
    # Continue with any other output hooks
  }
  hook_output(x, options)
})
suppressMessages(require(xtable))
suppressMessages(require(lattice))
suppressMessages(require(mgcv))
suppressMessages(require(qgam))
@
  
\begin{center} {\large \bf GAM modelling workshop: computer lab exercises}\end{center}

\begin{center} {Matteo Fasiolo and Simon N. Wood} \end{center}

The main packages we will need are \verb|mgcv|, \verb|mgcViz|, \verb|qgam| and \verb|mgcFam| which should have been installed by default by doing
<<install_0, echo = TRUE, cache=FALSE, eval=FALSE>>=
library(devtools)
install_github("mfasiolo/mgcFam")
install_github("mfasiolo/testGam")
@

All the data sets we will use in the workshop should now be available in your \verb|R| system.
The rest of the material for the workshop can be downloaded from \url{https://github.com/mfasiolo/workshop_RSS19}. If you download it as a zip file and extract it, the the solutions can be found in the \verb|exercises/solutions| folder.

% Please make sure that the following packages are installed:
% \begin{enumerate}
% \item \verb+devtools+ (\verb+install.packages("devtools")+);
% \item \verb+qgam+ (\verb+library(devtools); install_github("mfasiolo/qgam")+);
% \item \verb+mgcViz+ (\verb+library(devtools); install_github("mfasiolo/mgcViz")+);
% \item \verb+gamair+ (\verb+install.packages("gamair")+).
% \end{enumerate}

\section*{First session}

In the first session you could try one or more of the following exercises (suggested track is ex 2, 1 and potentially 3, and the number of \textbf{*} indicates the difficulty level):
\begin{enumerate}
\item Retinopathy among diabetics (sol: ``Retinopathy\_mgcv.html''). Simple exercise on basic GAM modelling in \verb|mgcv|. \textbf{*}
\item Modelling the simulated motorcycle accident data set (solution in: ``motorcycle\_mgcv.html''). Pedagogical exercise, using a 1D example to illustrate adaptive smooths, heteroscedastic data and location-shape GAM models. \textbf{**}
\item $\text{C0}_2$ modelling (sol: ``CO2\_mgcv.html''). Featuring cyclic seasonal smooths and the dangers of extrapolation. \textbf{**}
\item Ozone modelling (sol: ``Ozone\_mgcv.html''). Exercise focusing on manual variable selection via p-values and residual checking, and adjusting the mean-variance relationship. \textbf{*}
\end{enumerate}

\section*{Second session}

In the second session you could try one or more of the following exercises:
\begin{enumerate}
\setcounter{enumi}{4}
\item Forecasting electricity demand on GEFCom2014 data (solution in: ``gefcom\_small\_mgcv.html''). Simple exercise, focused on models building using residual checks and using only 1D effects. \textbf{*}
\item Larynx cancer in Germany (sol: ``Larynx\_mgcv.html''). Focused on spatial modelling using Markov Random Field, isotropic and tensor-product effects. \textbf{*}
\item Retinopathy among diabetics part 2  (sol: ``Retinopath\_mgcv\_2.html''). Features 2D smooth interactions, automatic variable selection and the use of GCV vs REML for smoothing parameter selection. \textbf{**}
\item Big GAM modelling of GEFCom14 electricity demand data (sol: ``gefcom\_big.html''). Featuring Big Data GAM methods and 2D tensor interactions.
\item Individual electricity demand modelling (solution in: ``Ind\_elect.html''). Featuring Big Data GAM methods and by-costumer smooth effects.
\item Mackerel egg data (sol: ``Mackerel.html''). Featuring 2D spatial interactions.
\item Bone mineral density modelling (sol: ``bone\_density.html''). Featuring simple random effects.
\end{enumerate}
but feel free to try \verb|mgcv| and \verb|mgcViz| on your own data. 

\vspace*{1\baselineskip}

Questions 4, 6, 7 and 9 have been adapted from Simon Wood's notes.

%Apologies to the linguists, epidemiologists and hydrologists among you: if you see that the analyses I propose in the examples do not make sense scientifically, let me know and I will correct the examples for future workshops.

\section{Retinopathy among diabetics: part 1} 

Data frame {\tt wesdr} (taken from Chong Gu's \verb|gss| package) contains a subset of data from a Wisconsin study on development of retinopathy among diabetics. The following variables are provided:
\begin{itemize}
\item {\tt ret}, a binary indicator of development of retinopathy by first follow up of study.
\item {\tt bmi}, the body mass index at entry to the study (between 18 and 25 is considered healthy).
\item {\tt dur}, the duration of diabetes, in years, at entry.
\item {\tt gly}, the percentage of glycosylated haemoglobin (HbA1C) in the blood (haemoglobin to which glucose has bound). 2.5-3.5\% is normal for non-diabetics. 6.5\% is generally considered good control for diabetics.
\end{itemize}

The aim is to model the probability of retinopathy as a function of the other variables.

Questions:

\begin{enumerate}
\item Load \verb+testGam+, \verb+mgcv+ and the data (\verb+data(wesrd)+). Have a look at the relation between the different variables using \verb+pairs(wesdr)+ or other exploratory visualisations.

\item Start by fitting a logistic regression model (\verb|family = binomial|), with smooth effects for duration and body mass index. Use \verb|summary| and \verb|plot| to verify whether the effects are strong.

\item Use \verb|qq.gam| to get a QQ-plot of the residuals (you can set the \verb|rep| argument to, say, 100 to get simulation-based reference intervals). Is the plot interpretable? Now plot the residuals against the values of \verb|wesdr$gly|, do you see any pattern?

\item Include a smooth effects for \verb|gly| in the model formula and refit. Looks at the fitted effects and verify their significance using \verb|plot| and \verb|summary|. Compare the new model to the first model you fitted using \verb|AIC|. Use \verb|gam.check| to produce further diagnostics. Does the text suggest that the you should modify the number of basis functions used? 
 
\end{enumerate} %% end of retinopathy question

\section{Simulated motorcycle accident} 

Here we consider the classic simulated motorcycle accident data set from the \verb|MASS| package. The data frame gives a series of measurements of head acceleration in a simulated motorcycle accident, used to test crash helmets. See \verb|?mcycle| for details. 

Questions:

\begin{enumerate}
\item Load \verb+mgcv+ and the data (\verb+library(MASS); data("mcycle")+). Have a look at the data using \verb|head| and a scatterplot.

\item Fit a Gaussian GAM, with acceleration as response and a smooth effect for \verb|times|. Obtain the fitted values and corresponding standard errors using \verb|predict|, and plot the fitted values with confidence intervals on top of a scatterplot of acceleration vs time. Do you see any issues with this fit?

\item Use \verb|gam.check| to get some diagnostics. Does the text output suggest that you should modify the number of basis function used? Try to increase the number of basis functions to 20, re-fit and re-check.

\item Now re-fit the model, using an adaptive basis for the effect of time \verb|s(accel, k = 20, bs = 'ad')|. Use the \verb|gam.check| or \verb|summary| to check the number of EDF used. Has it changed relative to the non-adaptive fit? If so, think about why this has happened. Compare the fitted effect of time under the adaptive and non-adaptive basis, do you see any differences?

\item Use \verb|qq.gam| to get a QQ-plot of the residuals (you can set the \verb|rep| argument to, say, 100 to get simulation-based reference intervals). Does this plot reveal any problem? Now produce a scatterplot of absolute residuals vs time, does the size of the residuals depend on time? The answer is `yes', and this is not taken into account by our model. We can address this 'manually' be estimating the variance of the residuals as a function of time, and use it to re-weight the observations when fitting the model (see next point). 

\item Regress the log squared residuals on \verb|times| using a Gaussian GAM. If we call this fit (say) \verb|resFit|, the resulting fitted values in \verb|resFit$fitted.values| are estimates of the expected value of the log squared residuals, as a function of time. Hence \verb|exp(resFit$fitted.values)| is an approximation to the residuals variance. Re-fit the acceleration vs time Gaussian GAM with weights inversely proportional to the variance (you have to use the \verb|weights| argument in \verb|gam|). Compare the resulted fitted effect and intervals with those you obtained under the un-weighted fit.   

\item \textbf{Extra:} Use the \verb|gaulss| family in \verb|mgcv| to fit a Gaussian GAM where the both the mean and the variance of the acceleration depend on time. See \verb|?gaulss| for details.
 
\end{enumerate} %% end of retinopathy question


\section{CO$_2$ modelling }
This question is about modelling data with seasonality, and the need to be very careful if trying to extrapolate with GAMs (or any statistical model). The data frame {\tt co2s} contains monthly measurements of CO$_2$ at the south pole from January 1957 onwards. The columns are {\tt co2}, the month of the year, {\tt month}, and the cumulative number of months since January 1957, {\tt c.month}. There are missing {\tt co2} observations in some months.\index{extrapolation!dangers of}

Questions:
\begin{enumerate}
\item Load \verb|mgcv| and the data with \verb+library(gamair); data(co2s)+
\item Plot the CO$_2$ observations against cumulative months.
\item Fit a Gaussian additive model with a smooth effect for \verb|c.month|, using the {\tt gam} function. Use the {\tt cr} basis, and a basis dimension of 100.  
\item Obtain the predicted CO$_2$ for each month of the data, plus 36 months after the end of the data, as well as associated standard errors. Produce a plot of the predictions with twice standard error bands. Are the predictions in the last 36 months credible? NB: to produce the plot you have to write your own code, \verb|mgcv| does not produce such plots.
\item Fit the model $\mathbb{E}(\text{CO}_2) = f_1({\tt c.month}_i) + f_2({\tt month}_i)$ where $f_1$ and $f_2$ are smooth functions. Use a basis of dimension 50 for $f_1$ and a cyclic basis for $f_2$. In the \verb|gam| call, you will need to set argument \verb+knots+ to \verb+list(month=c(1,13))+ to make so that that the effect of January is the same as January, not that December and January are the same!
\item Repeat the prediction and plotting in question 4 for the new model. Are the predictions more credible now? Explain the differences between the new results and those from question 4.
% \item Transform the fitted qgam model to a \verb|gamViz| object, by calling \verb|getViz| with argument \verb|nsim = 0| (we need to set \verb|nsim| to 0 because, at the present moment, qgam does not allow to simulate data from the model). Plot the smooth effects and experiment with model checks (e.g. QQ-plots). Can you expect the residuals to be normally distributed in a quantile regression model? We talk more about this in the exercise on electricity load forecasting.
\end{enumerate} %% C02

\section{Ozone modelling}

Data frame {\tt ozone} contains daily(ish) ozone  measurements over Los Angeles ({\tt O3}, ppm), along with:
\begin{trivlist}
\item {\tt vh} the height at which the atmospheric pressure is 500mb, in metres.
\item {\tt wind} the wind speed (reported as miles per hour, but this seems improbable).
\item {\tt humidity} (usual \% scale).
\item {\tt temp} air temperature (Fahrenheit).
\item {\tt ibh} the inversion layer base height in feet.
\item {\tt ibt} the inversion base temperature (Fahrenheit).
\item {\tt dpg} `Dagget air pressure gradient' (mmhg).
\item {\tt vis} visibility in miles.
\item {\tt doy} Julian day, where 1 is Jan 1 1976.
\end{trivlist}
The aim is to build a GAM model to explore the relationship between ozone and the other variables.

Questions:

\begin{enumerate}

\item Load \verb+testGam+ and the data (\verb+data(ozone)+), and use something like \verb+pairs(ozone)+ to have a look at it.

\item Load \verb+mgcv+ and use \verb|gam| to fit a Gaussian GAM with ${\tt O3}$ as response, where $\log(\mathbb{E}({\tt 03}))$ is given by a sum of smooth functions (e.g. \verb|s(wind)|) of each of the predictors. You will need to use the log-link, which requires using the argument \verb|family=gaussian(link=log)| in the call to \verb|gam|. Plot the fitted effects using \verb|plot|.

\item Check the model residuals using the \verb|gam.check| functions. Do you see any residual pattern when you plot the residuals against the fitted values or linear predictor?

\item Refit the model using a Gamma as response distribution (\verb|Gamma(link=log)|), and re-check the residuals. Does the residual distribution look better?

\item Fit an alternative model where you are using the identity-link (\verb|Gamma(link=identity)|). Does a model with an additive (i.e. identity-link) structure do better than that with a multiplicative (log-link) structure in terms of \verb|AIC|?  

\item Plot the smoothed effects again and use the \verb|summary| function to see which effects are significant. Try simplifying the model.

\item Once you have converged on a model, plot it and interpret the fitted smooth effects: do they make sense?

% \item {\bf Extra part} (only if time): The smooth function of {\tt doy} should arguably be cyclic (i.e. values and derivatives should match at `year ends'). Using the {\tt bs="cc"} option to {\tt s} can achieve this, but to get wrapping at the year end (rather than the data ends) you need to supply knot locations for {\tt doy}, which span a full year. To do this you can use the {\tt knots} argument to {\tt gam}: something like \verb+knots=list(doy=seq(25,390,length=10))+ should do it (the strange range is because the data span less than a year, but are spread over 2 calendar years).

\end{enumerate}

\section{Forecasting electricity demand on GEFCom2014 data}
Here we consider the electricity demand dataset taken from the GEFCom2014 challenge. The dataset covers the period January 2005 to December 2011 and it contains the following variables:
\begin{itemize}
\item \verb|NetDemand| net electricity demand between 11am and 12am.
\item \verb|wM| instantaneous temperature.
\item \verb|wM_s95| exponential smooth of \verb|wM|, that is \verb|wM_s95[i] = a*wM[i] + (1-a)*wM_s95[i]| with \verb|a=0.95|.
\item \verb|Posan| periodic index in \verb|[0, 1]| indicating the position along the year.
\item \verb|Dow| factor variable indicating the day of the week (I think that 0=Sunday and 6=Saturday, but I am not sure).
\item \verb|Trend| progressive counter, useful for defining the long term trend.
\item \verb|NetDemand.24| lagged version of \verb|NetDemand|, that is demand at the same time of the previous day.
\item \verb|Year| should be obvious.
\end{itemize}

Questions:
\begin{enumerate}
\item Load \verb+testGam+ and the data (\verb+data(gefcom_small)+). Have a look at it by, for instance, using \verb|pairs(gefcom_small)|.

\item Fit a Gaussian GAM where the model formula contains: smooth effects for \verb|wM|, \verb|wM_s95|, \verb|Posan| (optionally use a cyclic basis for the latter by doing \verb|s(Posan, bs="cc")|); parametric effects for \verb|Trend|, \verb|Dow| and \verb|NetDemand.24|. Plot the fitted effects using \verb|plot|, and look at the relative importance of the effects.

\item Plot the residuals against the \verb|Trend| variable, do you see any non-linear dependence (you might need to zoom in using \verb|ylim| because of an outlier)? Use \verb|gam.check| to check whether you should increase \verb|k| for any of the smooth effects. 

\item Increase \verb|k| for all effects, introduce a smooth effect for \verb|Trend|, the re-fit and repeat the checks in the previous point. Does everything look good? \verb|gam.check| shows that the effect of \verb|Trend| is using all the basis functions available, is this a problem? Once you have converged on a model, compare your new model to the old one in terms of \verb|AIC|.

\item Use \verb|qq.gam| to produce a QQ-plot of the residuals, do you see any problem? Refit the same model, but now use a scaled Student-t distributions by setting \verb|family = scat|. Any improvement in AIC? How do the residuals look now?

\item Check whether a scaled Student-t with log-link function \verb|scat(link=log)| achieves lower AIC. Then plot all the fitted effects of final model using \verb|plot| (you can set \verb|all.terms=TRUE| to plot also the parametric effects). Do the effects make sense?

\end{enumerate}

\section{Larynx cancer in Germany}

First load some data on cancer of the larynx by health reporting districts in Germany.
\begin{verbatim}
library(testGam)
library(mgcv)
data("Larynx")       # load Larynx cancer death data 'Larynx'
data("german.polys") # load polygons defining German regions 'german.polys'
# Get regions "midpoints"
X <- t(sapply(german.polys,colMeans,na.rm=TRUE))
\end{verbatim}
The variables in the \verb|Larynx| dataframe are:
\begin{itemize}
\item[region] code identifying region;
\item[E] expected number of deaths (according to population and pan German total);
\item[Y] number of deaths from Larynx cancer 1986-1990;
\item[x] measure of smoking rate in region.
\end{itemize}

Questions:

\begin{enumerate}
\item Run the code above and then use \verb|gam| to fit a Poisson GAM with a smooth effects for \verb|x| and the following Markov Random Field (MRF) effect for region:
\begin{verbatim}
s(region, k = 200, bs = "mrf", xt = list(polys=german.polys))
\end{verbatim}
and the offset term \verb|offset(log(E))|, meant to take into account the fact that the number of death is proportional to the population of each region. Plot the fitted effects.

\item Now substitute the MRF smooth either with the isotropic smooth \verb|s(X[,1],X[,2],k=200)|. Plot the 2D fitted effect in different ways using the \verb|scheme| argument (see \verb|plot.gam|) Which model does better in terms of AIC? 

\item Now use a tensor product smooth \verb|te(X[,1],X[,2],k=c(15, 15))| for the spatial effect. Plot it as before and compare the three spatial effects fitted so far. Which of the models does better in terms of AIC?

\item Check the last model we fitted using \verb|check.gam|. Do you get an error? This is because so far we adopted the bad practice of using global variables (\verb|X|) in our model formulas! Add each column of \verb|X| as a proper variable in the \verb|larynx| data set, modify the model formula accordingly and re-fit. Is the error gone? Now use the \verb|vis.gam| function to visualise the spatial effect in 3D. You can use the \verb|theta| and \verb|phi| arguments to modify the viewpoint (see \verb|?vis.gam|).

\end{enumerate}

\section{Retinopathy among diabetics (continued)} 

Data frame {\tt wesdr} (taken from Chong Gu's \verb|gss| package) contains a subset of data from a Wisconsin study on development of retinopathy among diabetics. The following variables are provided:
\begin{itemize}
\item {\tt ret}, a binary indicator of development of retinopathy by first follow up of study.
\item {\tt bmi}, the body mass index at entry to the study (between 18 and 25 is considered healthy).
\item {\tt dur}, the duration of diabetes, in years, at entry.
\item {\tt gly}, the percentage of glycosylated haemoglobin (HbA1C) in the blood (haemoglobin to which glucose has bound). 2.5-3.5\% is normal for non-diabetics. 6.5\% is generally considered good control for diabetics.
\end{itemize}

The aim is to model the probability of retinopathy as a function of the other variables.

Questions:

\begin{enumerate}
\item Load \verb+testGam+, \verb+mgcv+ and the data (\verb+data(wesrd)+). Have a look at the relation between the different variables using \verb+pairs(wesdr)+ or other exploratory visualisations.

\item In a previous exercise we found out that \verb|dur|, \verb|gly| and \verb|bmi| are all important predictors of retinopathy. Now we are looking for interaction of these variables. It is not immediately clear what interactions should appear in the linear predictor, hence in the first instance use all smooth main effects plus all two-way interactions using {\tt ti} terms with \verb|k=10|. Use a logistic regression model (\verb|family = binomial|). Use \verb|summary| verify which effects seems important and visualise them using \verb|plot|. Do you see any problem? (\textbf{NB}: here we are using a large number of basis functions, $(k-1) \times (k-1)$, for each smooth interaction to make a point.) 

\item Refit the same model, but now use \verb|method = "REML"| to select the smoothing parameters by REstricted Marginal Likelihood, rather than via the default Generalized Cross Validation (GCV) method.  Use \verb|summary| and \verb|plot| to check whether the model is still over-fitting.  

\item Refit the same model, but now use \verb|select = TRUE| to do automatic variable selection. Use \verb|summary| to check whether the EDF used by the interactions have changed, relative to the first fit. Has the shape of the interaction terms changed as well? 

\item Simplify the model by removing non-significant effects, re-fit and visualise the effects. Is the model with smooth interaction(s) better than a model with linear interactions (e.g. in terms of AIC)?
\end{enumerate} %% end of retinopathy question


\section{Big GAM modelling of GEFCom14 electricity demand data} 

Here we use again data from the GEFCom14 challenge, but this data set is 24 times larger than the one used in the previous exercise. This is because it contains data corresponding to all the 24 hourly slots. The variable \verb|Instant| indicates the hourly window corresponding to each row of the data set. All remaining covariates have the same interpretation as before.

Questions:
\begin{enumerate}
\item Load \verb+mgcViz+ and the data (\verb+load("data/gefcom_big.rda")+). Create a model formula with smooth effects \verb|wM|, \verb|wM_s95|, \verb|Instant|, \verb|Trend| and \verb|Posan|. Use regression splines bases (\verb|bs='cr'|) for all smooths apart from \verb|Posan|, for which you should use a cyclic basis (\verb|bs='cc'|). Use \verb|k = 6| for \verb|Trend| and \verb|k = 20| for \verb|Posan|. Leave \verb|k| to its default for the other smooths. Use parametric fixed effects for \verb|Dow| and \verb|NetDemand.24|. Use this formula within a \verb|bamV| call to fit a Gaussian GAM. When calling \verb|bamV| set \verb|aGam=list(discrete=TRUE)| to speed up computations (do this in all subsequent \verb|bamV| calls) and \verb|aViz = list(nsim = 50)| to perform the response simulations needed for residuals checking. Having fitted the model, look at the effects using \verb|plot| (recall that you can use argument \verb|allTerms=TRUE| to plot also the parametric effects).

\item Use \verb|check| to verify whether the number of basis functions used for the smooth effects is sufficiently large. Also, use the \verb|check1D| function with the \verb|l_gridCheck1D| layer to look for residual patterns across the variables.

\item Double \verb|k| for any of the effects where the number of basis functions seems to small, and re-fit. After re-fitting, check whether AIC has improved and repeat the residual checks.

\item We expect that several of the effects might depend on the time of day. Use the \verb|check2D| function with the \verb|l_gridCheck2D| layer to look for interactions between \verb|Instant| and \verb|NetDemand.24|, \verb|wM|, \verb|wM_s95| and \verb|Posan|. Notice that the binned mean residuals should ideally fall in the range (-2, 2) if the model was correct. Do you see any residual pattern? If so, fit a model which includes the necessary tensor product interactions (e.g. \verb|ti(wM, Instant, k = c(10, 10))|) and repeat the checks. Are the patterns still there?

\item Assuming that we are now satisfied with our model, we'll now have a detailed look at the fitted smooth effects. First, look at the marginal effects using the \verb|plot| function. Use the expression \verb|print(plot(fit2, select = ???), pages = 1)| to plot all the marginal effects on one page (substitute \verb|???| with the indexes of the univariate effects in your model). Do the same to plot the 2D interactions. Think about whether each effect makes physical sense to you. As an alternative to \verb|plot|, recall that you can extract any effect using the \verb|sm| function and produce a plot with customized layers. You can use the \verb|listLayers| function to get a list of the available layers. Then, use the \verb|plotRGL| function to manipulate each bivariate effect interactively.

\item \textbf{Extra question}: the model could be improved further. For instance, use the \verb|check2D| function with the \verb|l_gridCheck2D| layer to look at how the standard deviation and skewness of the residuals vary across pairs of covariates (the \verb|e1071| package provides a \verb|skewness| function, then you simply need to set \verb|gridFun = skewness| in the call to \verb|l_gridCheck2D|). Do you see any pattern? At this point we could consider a GAMLSS model with linear predictors for location, variance and skewness (e.g. the \verb|gaulss| or \verb|shash| family). However, \verb|bam| methods does not yet support such models, so you'll need to use \verb|gam| which can be much slower for large models.

\end{enumerate}




\section{Individual electricity demand modelling}
Here we consider electriciy demand from 28 commercial costumers. The dataset covers roughly three months and it contains the following variables:
\begin{itemize}
\item \verb|load| power usage from an individual costumer (in KW, I guess);
\item \verb|DateTime| the date and the time of day;
\item \verb|instant| the time of day, where 1 corresponds to 00:00-00:30, 2 to 00:30-01:00 and so on;
\item \verb|dow| factor variable indicating the day of the week;
\item \verb|temp| instantaneous temperature;
\item \verb|tempL| exponential smooth of \verb|temp|, that is \verb|tempL[i] = a*temp[i] + (1-a)*tempL[i-1]| with \verb|a=0.95|;
\item \verb|ID| the unique ID of each individual costumer;
\item \verb|load48SM| lagged version of smoothed \verb|load|, where the smoothing was performed as for \verb|tempL|.
\item \verb|day| a counter depending on the day.
\end{itemize}

Questions:
\begin{enumerate}
\item Load \verb+mgcViz+ and the data (\verb+load("data/Ind_elect.rda")+). Then use \verb|bamV| to fit a Gaussian GAM model with smooth effects for \verb|instant|,  \verb|temp| and \verb|day|, and parametric effects for \verb|dow| and \verb|ID|. In the call to \verb|bamV| set \verb|aViz = list(nsim = 50)| to perform the response simulations needed for residuals checking. Look at the model output using \verb|plot| and \verb|summary|.

\item Now we start looking for interactions. Use the \verb|check2D| function with the \verb|l_gridCheck2D| layer to look for interactions between \verb|ID| and \verb|instant|,  \verb|temp| and \verb|day|. Notice that the binned mean residuals should ideally fall in the range (-2, 2), if the model is correct. Do you see large deviation? If so for which costumer(s) in particular?

\item Modify the model formula to include by-factor smooths, that is \verb|s(instant, by = ID, id = 1)| \verb|s(temp, by = ID, id = 2)| and \verb|s(day, by = ID, id = 3)|. The \verb|id| argument make so that each of the 3 by-factor smooths has its own smoothing parameter, but the same smoothing parameter is used across all costumers. Refit the model using \verb|bamV|, and set the argument \verb|aGam=list(discrete=TRUE)| to speed up computation by discretisation. Compare this models to the previous one using AIC, and repeat the residuals checks. Any improvement?

\item Use \verb|check| to verify whether the number of basis functions used for the smooth effects is sufficiently large. Double \verb|k| for any of the effects where the number of basis functions seems to small, and re-fit. After re-fitting, check whether AIC has improved.

\item Use the \verb|check2D| function with the \verb|l_gridCheck2D| layer to look for interactions between \verb|ID| and \verb|load48SM|. If the effect of \verb|load48SM| seems important, include the corresponding by-factor smooth by adding \verb|s(load48SM, by = ID, id = 4)| to the model and re-fit. 

\item Look at the model output using \verb|plot|, using the \verb|select| argument to plot any specific effect (you can't plot them all together, because the model includes tens of them). Compare the consumption of some of the individual costumers with the model predictions (which you can find in \verb|fittedModel$fitted.values|). Do some costumers look much harder to predict than others?

\end{enumerate}


\section{Mackerel egg data} 

The following code loads and plots some data from a fish egg survey, for the purposes of spatial modelling.  
\begin{verbatim}
library(mgcViz); load("data/mack.rda"); load("data/coast.rda")
## plot data....
with(mack,plot(lon,lat,cex=0.2+egg.dens/150,col="red"))
lines(coast)
ind <- c(1,3,4,5,10,11,16) 
pairs(mack[,ind])
\end{verbatim}

The main variables of interest in the \verb|mack| data set are:
\begin{itemize}
\item \verb|egg.count| number of eggs found in the net;
\item \verb|c.dist| distance from 200m seabed contour;
\item \verb|b.depth| depth of the ocean;
\item \verb|temp.surf| surface temperature of the ocean;
\item \verb|temp.20m| water temperature at a depth of 20 meters;
\item \verb|lat| latitude;
\item \verb|lon| longitude;
\item \verb|salinity|;
\item \verb|net.area| the area of the net used in $\text{m}^2$.
\end{itemize}

Questions:

\begin{enumerate}
\item Use the code above to load and plot the data;
\item Create a new variable \verb|mack$log.net.area <- log(mack$net.area)|, and use \verb|gamV| to fit a Poisson GAM with \verb|egg.count| as response variable and 1D smooth effects for all the other variables, with the exceptions of \verb|net.area| and \verb|log.net.area|. Instead, include in the model formula the term \verb|offset(log.net.area)|, meant to take into account the fact that the number of eggs captured is proportional to the net area. 

\item Look at the model residuals using \verb|qq|. What kind of problem do you see? Re-fit the models using a negative binomial (\verb|family=nb|) or Tweedie (\verb|family=tw|) response distribution, and check which model is better in terms of residuals QQ-plots and AIC.

\item Let \verb|fit| be the best of the three GAM models you just fitted. Use \verb|fit<-getViz(fit,nsim=50)| to get some simulated residuals, and then use the \verb|check2D| function with the \verb|l_gridCheck2D| layer to look for residual patters across \verb|lon| and \verb|lat|. Then refit the model using a bivariate isotropic effect \verb|s(lon, lat, k=100)|, re-check the residuals and see whether AIC has improved.

\item Use \verb|check| to verify whether the number of basis functions used for the smooth effects is sufficiently large. Then use the \verb|check1D| function with the \verb|l_gridCheck1D| layer look for residual patterns across some of the variables. If necessary, modify the model.

\item Plot the fitted effects using \verb|plot|. Which effects look more important (look at the scales)? Use the \verb|plotRGL| function to manipulate spatial effect interactively.

% \item \textbf{Extra question}: you can verify whether the effect of \verb|b.depth| depends on the spatial location, by adding the interaction term \verb|s(lon, lat, by = I(b.depth))| to the model. Fit the corresponding model with \verb|gamV| and visualize the 2D interaction term.
\end{enumerate}



\section{Bone mineral density modelling}
This dataset is taken from the package \verb|lava|. It consists of 112 girls randomized to receive calcium or placebo. The response variable of interests consists of longitudinal measurements of bone mineral density ($g/cm^2$) measured approximately every 6th month for 3 years. All girls are approximately 11yo at the start of the trial. The main variables are:
\begin{itemize}
\item \verb|bmd| bone mass density;
\item \verb|group| placebo or supplement;
\item \verb|person| factor indicating the id of each girl;
\item \verb|age| the age of each girl at the time of each measurement;
\end{itemize}

Questions:
\begin{enumerate}
\item Load \verb|mgcViz| and the data with \verb+load("data/calcium.rda")+. Then use \verb|gamV| to fit a Gaussian GAM model with \verb|bmd| as response and linear effects for \verb|age| and \verb|group|. In the call to \verb|gamV| set the argument \verb|aViz=list(nsim = 50)| to have some simulated responses for residuals checks. Use \verb|summary| to print the model output. Is the placebo effect significant? (which is the same as asking whether the treatment effect is significant)
\item Use \verb|check1D| with the \verb|l_gridCheck1D| layer to check that the mean of the negative residuals does not depart too much from 0, for any of the subjects. If you see significant departures add a random effect for \verb|person| to the models formula (\verb|s(person, bs="re")|), then re-fit and re-check the residuals. Print the model output again using \verb|summary|.
\item Now modify the model formula to use a smooth effect for \verb|age|, and plot the fitted effects using \verb|plot|. Use the function \verb|AIC| to compare the model with a smooth effects for \verb|age| with the model which uses a linear \verb|age| effect.
\item Verify whether the smooth age effect is different between the placebo and the treatment group, by using a by-factor smooth. To do this substitute \verb|s(age)| with \verb|s(age, by=group)| in the model formula, refit and then plot the fitted effects. To see the difference between the two smooths more clearly, use the \verb|plotDiff| function with the \verb|l_fitLine| and \verb|l_ciLine| layers.
\end{enumerate} 


\section{Retinopathy among diabetics} 

Data frame {\tt wesdr} contains a subset of data from a Wisconsin study on development of retinopathy among diabetics. The following variables are provided:
\begin{itemize}
\item {\tt ret}, a binary indicator of development of retinopathy by first follow up of study.
\item {\tt bmi}, the body mass index at entry to the study (between 18 and 25 is considered healthy).
\item {\tt dur}, the duration of diabetes, in years, at entry.
\item {\tt gly}, the percentage of glycosylated haemoglobin (HbA1C) in the blood (haemoglobin to which glucose has bound). 2.5-3.5\% is normal for non-diabetics. 6.5\% is generally considered good control for diabetics.
\end{itemize}

The aim is to model the probability of retinopathy as a function of the other variables.

Questions:

\begin{enumerate}
\item Load \verb+mgcViz+ and the data (\verb+wesrd <- read.table("data/wesdr.txt")+), and use \verb+pairs(wesdr)+ to look at it.

\item It is not immediately clear what model structure for the linear predictor is appropriate. So in the first instance try all smooth main effects plus two way interactions using {\tt ti} terms. Use a logistic regression model (\verb|family = binomial|). In the call to \verb|gamV| set \verb|aViz = list(nsim = 50)| to do some residual simulations, and \verb|aGam = list(select=TRUE)| to do some variable selection. Use \verb|summary| and \verb|plot| to verify which effects seems important.

\item Simplify the model by removing non-significant effects and re-fit. Check the model residuals using QQ-plots of normally transformed residuals (\verb|qq(yourFit, type = "tnorm")|) as well as the \verb|check1D| function with the \verb|l_gridCheck1D| layer look for residual patterns across the individual variables.

\item Use the \verb|plotRGL| function to visualize and manipulate any bi-variate effect in your model interactively.

\end{enumerate} %% end of retinopathy question








\section{Larynx cancer in Germany}

First load some data on cancer of the larynx by health reporting districts in Germany.
\begin{verbatim}
library(mgcViz)
load("data/german.polys.rda") ## load polygons defining German regions 'german.polys'
load("data/Larynx.rda")       ## load Larynx cancer death data 'Larynx'
## Get regions "midpoints"....
X <- t(sapply(german.polys,colMeans,na.rm=TRUE))
\end{verbatim}
The variables in the \verb|Larynx| dataframe are:
\begin{itemize}
\item[region] code identifying region;
\item[E] expected number of deaths (according to population and pan German total);
\item[Y] number of deaths from Larynx cancer 1986-1990;
\item[x] measure of smoking rate in region.
\end{itemize}

Questions:

\begin{enumerate}
\item Run the code above and then use \verb|gamV| to fit a Poisson GAM with a smooth effects for \verb|x| and the following Markov Random Field (MRF) effect for region:
\begin{verbatim}
s(region, k = 200, bs = "mrf", xt = list(polys=german.polys))
\end{verbatim}
and the offset term \verb|offset(log(E))|, meant to take into account the fact that the number of death is proportional to the population of each region. In the call to \verb|gamV| set \verb|aViz = list(nsim = 50)| to do some residual simulations for later checking. Plot the fitted effects.

\item Now substitute the MRF smooth either with the isotropic smooth \verb|s(X[,1],X[,2],k=200)| or with the tensor product smooth \verb|te(X[,1],X[,2],k=c(15, 15))|. Plot the corresponding fitted effects and compare them. Which model does better in terms of AIC?

\item Use the \verb|plotRGL| function to visualize and compare the fitted isotropic and tensor product smooth. You can use the code
\begin{verbatim}
mfrow3d(1, 2)
plotRGL(sm(fit2,1), residuals = TRUE)
next3d()
plotRGL(sm(fit3,1), residuals = TRUE)
\end{verbatim}
to get two rgl plots side by side.

\end{enumerate}


\end{document}
